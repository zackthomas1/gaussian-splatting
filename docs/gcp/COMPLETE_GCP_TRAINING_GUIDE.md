# Complete Guide: Training 3D Gaussian Splatting Models on GCP

This comprehensive guide covers everything you need to train high-quality 3D Gaussian Splatting models using Google Cloud Platform GPU instances, from VM setup to visualization of results.

## Table of Contents
1. [Understanding the Project Structure](#understanding-the-project-structure)
2. [Prerequisites](#prerequisites)
3. [VM Instance Setup](#vm-instance-setup)
4. [Environment Configuration](#environment-configuration)
5. [Training Workflows](#training-workflows)
6. [Troubleshooting](#troubleshooting)
7. [Best Practices](#best-practices)

---

## Understanding the Project Structure

### Core Components

The Gaussian Splatting codebase consists of four main systems:

#### 1. **Optimizer** (`train.py`)
- **Purpose**: PyTorch-based training system that produces 3D Gaussian models from Structure-from-Motion (SfM) inputs
- **Key Features**:
  - Iterative optimization of 3D Gaussian parameters (position, rotation, scaling, opacity, color)
  - Densification and pruning of Gaussians during training
  - Support for multiple optimizer types (default Adam, sparse Adam for acceleration)
  - TensorBoard logging for monitoring training progress
  - Checkpoint saving at configurable intervals
- **Requirements**: CUDA-ready GPU with 24GB VRAM recommended

#### 2. **Data Converter** (`convert.py`)
- **Purpose**: Processes raw images/videos into COLMAP-based SfM reconstructions
- **What it does**:
  - Extracts SIFT features from images
  - Matches features between image pairs
  - Reconstructs camera positions and sparse 3D point cloud
  - Creates the `sparse/` directory structure required for training
- **Dependencies**: COLMAP, optionally FFmpeg for video processing

#### 3. **Renderer** (`render.py`)
- **Purpose**: Generates rendered images from trained models
- **Outputs**:
  - `train/` folder: Renderings of training camera viewpoints
  - `test/` folder: Renderings of test camera viewpoints
  - Saved as PNG images for easy inspection
- **Use Case**: Quality evaluation and creating flythrough sequences

#### 4. **Real-Time Viewer** (`SIBR_viewers/`)
- **Purpose**: OpenGL-based interactive 3D visualization
- **Note**: Requires local machine with GUI (cannot run on headless VM)
- **Features**: Free camera movement, real-time rendering at high frame rates

### Critical Directory Structure

```
gaussian-splatting/
‚îú‚îÄ‚îÄ data/                          # Your datasets go here
‚îÇ   ‚îî‚îÄ‚îÄ <scene_name>/
‚îÇ       ‚îú‚îÄ‚îÄ input/                 # Source images (REQUIRED)
‚îÇ       ‚îú‚îÄ‚îÄ sparse/                # COLMAP output (auto-generated by convert.py)
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ 0/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ cameras.bin
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ images.bin
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ points3D.bin
‚îÇ       ‚îî‚îÄ‚îÄ distorted/             # Intermediate COLMAP files
‚îÇ
‚îú‚îÄ‚îÄ output/                        # Training outputs
‚îÇ   ‚îî‚îÄ‚îÄ <random_uuid>/             # Each training run gets unique ID
‚îÇ       ‚îú‚îÄ‚îÄ point_cloud/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ iteration_7000/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ iteration_30000/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ       ‚îú‚îÄ‚îÄ cameras.json           # Camera metadata
‚îÇ       ‚îî‚îÄ‚îÄ cfg_args              # Training configuration
‚îÇ
‚îú‚îÄ‚îÄ submodules/                    # CUDA extensions (must be compiled)
‚îÇ   ‚îú‚îÄ‚îÄ diff-gaussian-rasterization/  # Core rasterization kernel
‚îÇ   ‚îú‚îÄ‚îÄ simple-knn/                   # KNN for Gaussian initialization
‚îÇ   ‚îî‚îÄ‚îÄ fused-ssim/                   # Perceptual loss computation
‚îÇ
‚îú‚îÄ‚îÄ scene/                         # Data loading and scene representation
‚îÇ   ‚îú‚îÄ‚îÄ dataset_readers.py        # COLMAP and Blender format parsers
‚îÇ   ‚îú‚îÄ‚îÄ gaussian_model.py         # 3D Gaussian parameter storage
‚îÇ   ‚îî‚îÄ‚îÄ cameras.py                # Camera model definitions
‚îÇ
‚îú‚îÄ‚îÄ utils/                         # Helper functions
‚îÇ   ‚îú‚îÄ‚îÄ loss_utils.py             # L1, SSIM loss implementations
‚îÇ   ‚îú‚îÄ‚îÄ graphics_utils.py         # 3D math utilities
‚îÇ   ‚îî‚îÄ‚îÄ system_utils.py           # File I/O, checkpointing
‚îÇ
‚îî‚îÄ‚îÄ arguments/                     # Command-line parameter definitions
    ‚îî‚îÄ‚îÄ __init__.py               # ModelParams, OptimizationParams, etc.
```

### Key Configuration Parameters

When running `train.py`, these are the most important parameters:

- **`-s, --source_path`**: Path to your scene data (e.g., `data/myscene`)
- **`-m, --model_path`**: Output directory (default: auto-generated in `output/`)
- **`--iterations`**: Total training steps (default: 30,000)
- **`--eval`**: Split data into train/test sets for evaluation
- **`--resolution`**: Downsample images (e.g., `-r 2` for 50% resolution)
- **`--white_background`**: Use white instead of black background

---

## Prerequisites

### Required on Local Machine
- Google Cloud Platform account with billing enabled
- `gcloud` CLI installed ([Install Guide](https://cloud.google.com/sdk/docs/install))
- Authenticated to GCP: `gcloud auth login`

### Required Knowledge
- Basic Linux command line usage
- Understanding of SSH and file transfer concepts
- Familiarity with conda/Python environments (helpful but not required)

---

## VM Instance Setup

### Step 1: Create GPU-Enabled VM

#### Recommended Configuration

For **full-quality training** (paper-level results):
```
Machine Type: g2-standard-4
GPU: NVIDIA L4 (24GB VRAM) - included with G2 series
Region: us-west1 (Oregon) or us-central1 (Iowa)
vCPUs: 4
RAM: 16 GB
Boot Disk: 100-200 GB SSD
```

For **budget/experimental training**:
```
Machine Type: n1-standard-4
GPU: NVIDIA T4 (16GB VRAM) - attach manually
Region: Any with GPU availability
Note: Reduce batch size or resolution for smaller scenes
```

#### Detailed Creation Steps

1. **Navigate to VM Instances**
   ```
   GCP Console ‚Üí Compute Engine ‚Üí VM Instances ‚Üí CREATE INSTANCE
   ```

2. **Configure Basic Settings**
   - **Name**: `gaussian-splatting-vm`
   - **Region**: `us-west1` (Oregon)
   - **Zone**: `us-west1-b` (check GPU availability)

3. **Select Machine Type**
   - **Series**: `G2` (for L4 GPU)
   - **Machine type**: `g2-standard-4`
   - The L4 GPU (24GB) is automatically included

4. **Configure Boot Disk** ‚ö†Ô∏è CRITICAL STEP
   - Click **CHANGE** under Boot disk
   - **Operating System**: Select `Deep Learning on Linux`
   - **Version**: Choose `Deep Learning VM with CUDA 11.8 M115` (or latest with CUDA 11.8)
     - ‚ö†Ô∏è **DO NOT** select CUDA 12.x - incompatible with project dependencies
   - **Boot disk type**: `SSD persistent disk` (recommended)
   - **Size**: `100 GB` minimum (200 GB for multiple scenes)
   - Click **SELECT**

5. **Firewall**
   - ‚úì Allow HTTP traffic
   - ‚úì Allow HTTPS traffic
   - (Optional for network viewer access)

6. **Create**
   - Click **CREATE**
   - Wait 1-2 minutes for provisioning

#### Handling GPU Quota Limits

**Error**: `The GPUS-ALL-REGIONS-per-project quota maximum has been exceeded`

**Solution**:
1. Go to **IAM & Admin** ‚Üí **Quotas**
2. Filter: `gpus_all_regions`
3. Select `compute.googleapis.com/gpus_all_regions`
4. Click **EDIT QUOTAS**
5. Request new limit: `1` (or more if needed)
6. Provide justification: "Training 3D Gaussian Splatting models for research"
7. Submit request
   - Approval is typically instant for small increases
   - May take 24-48 hours for new accounts or large requests

### Step 2: Connect to VM

#### Option A: Browser SSH (Easiest)
1. In the VM Instances list, click **SSH** button next to your VM
2. A new browser window opens with terminal access

#### Option B: gcloud CLI (Recommended for file transfers)
```bash
gcloud compute ssh gaussian-splatting-vm --zone=us-west1-b
```

---

## Environment Configuration

### Step 1: Verify GPU and CUDA

Once connected to your VM, verify the GPU is detected:

```bash
# Check GPU
nvidia-smi
```

**Expected Output**:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.xx.xx    Driver Version: 535.xx.xx    CUDA Version: 11.8    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA L4           Off  | 00000000:00:03.0 Off |                    0 |
| N/A   XX¬∞C    P0    XX W / 72W|      0MiB / 23034MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

**Troubleshooting**: If GPU is not shown:
```bash
# Check if driver loaded
lsmod | grep nvidia

# If empty, install NVIDIA drivers
sudo apt-get update
sudo apt-get install -y nvidia-driver-535
sudo reboot
```

### Step 2: Install System Dependencies

```bash
# Update package manager
sudo apt-get update

# Install COLMAP (for Structure-from-Motion)
sudo apt-get install -y colmap

# Install FFmpeg (for video processing)
sudo apt-get install -y ffmpeg

# Install git (usually pre-installed)
sudo apt-get install -y git

# Verify installations
colmap help
ffmpeg -version
```

### Step 3: Clone Repository

```bash
# Clone with submodules (CRITICAL: don't forget --recursive)
cd ~
git clone --recursive https://github.com/graphdeco-inria/gaussian-splatting.git
cd gaussian-splatting
```

**Why `--recursive`?**: The project depends on three CUDA extension submodules:
- `diff-gaussian-rasterization`: Core rendering engine
- `simple-knn`: K-nearest neighbors for initialization
- `fused-ssim`: Fast SSIM loss computation

### Step 4: Configure Conda Environment

The Deep Learning VM includes Conda. Add it to your PATH if needed:

```bash
# Test if conda is available
which conda

# If not found, add to PATH
export PATH=/opt/conda/bin:$PATH
conda init
source ~/.bashrc
```

#### Method 1: Manual Setup (Recommended for L4/G2)

This method ensures compatibility with the L4 GPU's Ada Lovelace architecture:

```bash
# Create environment with Python 3.8
conda create -n gaussian_splatting python=3.8 -y

# Activate environment
conda activate gaussian_splatting

# Install PyTorch 2.0 with CUDA 11.8 (REQUIRED for L4 GPU)
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118

# Install Python dependencies
pip install plyfile tqdm opencv-python joblib

# Install CUDA extensions (ensure you're in gaussian-splatting directory)
cd ~/gaussian-splatting
pip install ./submodules/diff-gaussian-rasterization
pip install ./submodules/simple-knn
pip install ./submodules/fused-ssim
```

**Why PyTorch 2.0?**: The default `environment.yml` uses PyTorch 1.12, which lacks support for newer GPUs (Compute Capability 8.9+) like the L4.

#### Method 2: Using environment.yml (For T4 or older GPUs)

If using a T4 GPU or older architecture:

```bash
# Edit the environment file for CUDA 11.8
nano environment.yml
```

Modify these lines:
```yaml
# Before:
python=3.7.13
cudatoolkit=11.6
pytorch=1.12.1

# After:
python=3.8
cudatoolkit=11.8
pytorch=2.0.1
```

Save (Ctrl+O, Enter) and exit (Ctrl+X), then:

```bash
conda env create --file environment.yml
conda activate gaussian_splatting
```

### Step 5: Verify Installation

```bash
# Activate environment if not already active
conda activate gaussian_splatting

# Test PyTorch CUDA
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"
```

**Expected Output**:
```
PyTorch version: 2.0.1
CUDA available: True
CUDA version: 11.8
GPU: NVIDIA L4
```

---

## Training Workflows

### Workflow A: Training with Sample Datasets

Perfect for testing the setup and understanding the process.

#### Download Sample Data

```bash
cd ~/gaussian-splatting
mkdir -p data
cd data

# Download Tanks & Temples + Deep Blending dataset (650 MB)
wget https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/datasets/input/tandt_db.zip

# Extract
unzip tandt_db.zip
```

#### Available Scenes
After extraction, you'll have:
```
data/tandt/
‚îú‚îÄ‚îÄ train/      # Single scene from Tanks & Temples
‚îú‚îÄ‚îÄ truck/      # Another T&T scene
‚îî‚îÄ‚îÄ ...
```

#### Run Training

```bash
cd ~/gaussian-splatting
conda activate gaussian_splatting

# Train on the 'train' scene
python train.py -s data/tandt/train
```

**What happens during training:**
1. **Initialization (0-500 iterations)**: Creates initial 3D Gaussians from SfM point cloud
2. **Densification (500-15,000 iterations)**: Adds/removes Gaussians based on image gradients
3. **Refinement (15,000-30,000 iterations)**: Fine-tunes Gaussian parameters
4. **Checkpointing**: Saves model at iterations 7,000, 30,000, and final

**Expected Training Time**:
- L4 GPU: ~20-30 minutes for 30,000 iterations
- T4 GPU: ~40-60 minutes

**Monitoring Progress**:
```bash
# In another SSH session or terminal pane
cd ~/gaussian-splatting/output/<your_model_folder>
tensorboard --logdir=. --host=0.0.0.0 --port=6006
```

Then on your local machine, set up SSH tunnel:
```bash
gcloud compute ssh gaussian-splatting-vm --zone=us-west1-b -- -L 6006:localhost:6006
```
Open `http://localhost:6006` in your browser.

### Workflow B: Training with Custom Images

Use your own photos to create 3D models.

#### Prepare Image Data

**Requirements**:
- 50-300 images (optimal: 100-200)
- High resolution (1920x1080 or higher)
- Good coverage of the scene from multiple angles
- Overlap between consecutive images (>70%)
- Consistent lighting conditions

**Naming convention**: Any format (JPEG, PNG) with sequential naming helps organization.

#### Upload Images to VM

On your **local machine**:

```bash
# Upload folder of images
gcloud compute scp --recurse /path/to/your/images/ gaussian-splatting-vm:~/gaussian-splatting/data/myscene_images/
```

#### Process Images on VM

```bash
cd ~/gaussian-splatting
conda activate gaussian_splatting

# Create dataset structure
mkdir -p data/myscene/input

# Move images to input folder
mv data/myscene_images/* data/myscene/input/

# Run COLMAP reconstruction
python convert.py -s data/myscene
```

**What `convert.py` does**:
1. Extracts SIFT features from each image (~1-2 seconds per image)
2. Matches features between all image pairs (~N¬≤ complexity)
3. Reconstructs camera poses using bundle adjustment
4. Generates sparse 3D point cloud
5. Creates `data/myscene/sparse/0/` directory with COLMAP binary files

**Expected Processing Time**:
- 50 images: 5-10 minutes
- 100 images: 15-30 minutes
- 200 images: 30-60 minutes

**Troubleshooting COLMAP Errors**:

If you get display-related errors:
```bash
# Install virtual display
sudo apt-get install -y xvfb

# Run with virtual display
xvfb-run -a python convert.py -s data/myscene
```

If feature matching fails (too few matches):
```bash
# Try sequential matching instead of exhaustive
python convert.py -s data/myscene --skip_matching
cd data/myscene
colmap sequential_matcher --database_path distorted/database.db
colmap mapper --database_path distorted/database.db --image_path input --output_path sparse
```

#### Train on Custom Data

```bash
python train.py -s data/myscene
```

**Recommended flags for custom data**:
```bash
# Full quality, with train/test split
python train.py -s data/myscene --eval

# Reduce resolution if images are very large (speeds up training)
python train.py -s data/myscene -r 2  # 50% resolution

# Extend training for better quality
python train.py -s data/myscene --iterations 50000
```

### Workflow C: Training with Custom Video

Convert video footage into 3D models.

#### Upload Video to VM

On your **local machine**:
```bash
gcloud compute scp /path/to/your/video.mp4 gaussian-splatting-vm:~/gaussian-splatting/
```

#### Extract Frames

On the **VM**:

```bash
cd ~/gaussian-splatting

# Create dataset structure
mkdir -p data/myscene/input

# Extract frames (adjust FPS based on video length)
ffmpeg -i video.mp4 -qscale:v 1 -qmin 1 -vf "fps=2" data/myscene/input/%04d.jpg
```

**FPS Guidelines**:
- **Slow pan/static camera**: 1-2 FPS
- **Walking speed**: 2-4 FPS  
- **Fast motion**: 5-10 FPS
- **Goal**: 100-300 total frames with good overlap between consecutive frames

**Example**:
- 30-second video at 2 FPS ‚Üí 60 frames
- 60-second video at 2 FPS ‚Üí 120 frames (good)
- 120-second video at 3 FPS ‚Üí 360 frames (may be too many, increase GPU memory usage)

#### Process and Train

```bash
# Run COLMAP
python convert.py -s data/myscene

# Train
python train.py -s data/myscene --eval
```

---

## Post-Training: Rendering and Visualization

### Render Images

After training completes, generate rendered views:

```bash
cd ~/gaussian-splatting

# List output directories
ls output/

# Render the trained model (replace <uuid> with your model's folder name)
python render.py -m output/<uuid>
```

**Output structure**:
```
output/<uuid>/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îî‚îÄ‚îÄ ours_30000/
‚îÇ       ‚îú‚îÄ‚îÄ renders/     # Rendered images from training views
‚îÇ       ‚îî‚îÄ‚îÄ gt/          # Ground truth images
‚îî‚îÄ‚îÄ test/
    ‚îî‚îÄ‚îÄ ours_30000/
        ‚îú‚îÄ‚îÄ renders/     # Rendered images from test views
        ‚îî‚îÄ‚îÄ gt/          # Ground truth images
```

### Evaluate Quality Metrics

```bash
# Calculate PSNR, SSIM, LPIPS metrics
python metrics.py -m output/<uuid>
```

**Good quality benchmarks**:
- PSNR > 25 dB (higher is better)
- SSIM > 0.85 (max is 1.0)
- LPIPS < 0.15 (lower is better)

### Download Results to Local Machine

On your **local machine**:

```bash
# Download entire model folder
gcloud compute scp --recurse gaussian-splatting-vm:~/gaussian-splatting/output/<uuid> ./local-models/

# Or just download rendered images
gcloud compute scp --recurse gaussian-splatting-vm:~/gaussian-splatting/output/<uuid>/train/ours_30000/renders ./local-renders/
```

### View with SIBR Real-Time Viewer (Local)

If you have the SIBR viewer compiled locally:

```bash
# On your local machine
./SIBR_gaussianViewer_app -m ./local-models/<uuid>
```

**Controls**:
- Mouse drag: Rotate camera
- Scroll: Zoom in/out
- WASD: Move camera position
- Space: Toggle between free camera and original cameras

---

## Troubleshooting

### Training Issues

#### Out of Memory Errors

**Symptom**: `CUDA out of memory` during training

**Solutions**:
```bash
# 1. Reduce image resolution
python train.py -s data/myscene -r 4  # 25% resolution

# 2. Reduce batch size (edit train.py)
# 3. Use fewer images (remove some from input/ folder)
# 4. Upgrade to VM with more VRAM
```

#### Training Diverges (Loss ‚Üí NaN)

**Symptom**: Loss becomes `nan` after some iterations

**Solutions**:
```bash
# 1. Check for corrupted images
identify -verbose data/myscene/input/*.jpg | grep -i error

# 2. Ensure COLMAP reconstruction succeeded
ls data/myscene/sparse/0/
# Should contain: cameras.bin, images.bin, points3D.bin

# 3. Reduce learning rates
python train.py -s data/myscene --position_lr_init 0.00008
```

### COLMAP Issues

#### Few Features Detected

**Symptom**: `convert.py` fails with "insufficient matches"

**Solutions**:
```bash
# 1. Check image quality
identify data/myscene/input/*.jpg | head

# 2. Try different camera model
python convert.py -s data/myscene --camera SIMPLE_RADIAL

# 3. Increase image overlap (extract more frames from video)
```

#### Bundle Adjustment Fails

**Symptom**: COLMAP mapper fails to reconstruct scene

**Solutions**:
```bash
# 1. Use sequential matching for video-derived images
python convert.py -s data/myscene --skip_matching
# Then manually run sequential matcher (see Workflow B above)

# 2. Ensure sufficient baseline between images (not too close together)
```

### Environment Issues

#### ImportError: cannot import name 'simple_knn'

**Solution**:
```bash
cd ~/gaussian-splatting
conda activate gaussian_splatting
pip install --force-reinstall ./submodules/simple-knn
```

#### CUDA version mismatch

**Symptom**: `RuntimeError: CUDA error: no kernel image is available for execution`

**Solution**: Ensure PyTorch CUDA version matches system CUDA:
```bash
# Check system CUDA
nvidia-smi  # Look at CUDA Version

# Check PyTorch CUDA
python -c "import torch; print(torch.version.cuda)"

# Reinstall matching PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

## Best Practices

### Data Acquisition Tips

1. **Image Capture**:
   - Use consistent camera settings (manual mode preferred)
   - Maintain consistent lighting (avoid mixed daylight/artificial)
   - Capture in RAW if possible, convert to high-quality JPEG
   - Aim for 70-80% overlap between consecutive images

2. **Scene Considerations**:
   - **Good scenes**: Static objects, textured surfaces, good lighting
   - **Challenging scenes**: Reflective surfaces, transparent objects, uniform textures
   - **Avoid**: Moving objects, extreme lighting changes, motion blur

3. **Camera Coverage**:
   - Capture from multiple heights and angles
   - Complete 360¬∞ coverage if possible
   - Include close-up and wide shots
   - For videos: move slowly and smoothly

### Training Optimization

1. **Start Small**: Test with low resolution (`-r 4`) to validate dataset before full training

2. **Use Evaluation Split**: Always train with `--eval` flag for train/test metrics

3. **Monitor Training**:
   ```bash
   # Watch training progress
   tail -f output/<uuid>/training_log.txt
   ```

4. **Incremental Approach**:
   - Train to 7,000 iterations first (checkpoint saved automatically)
   - Check intermediate results
   - Continue to 30,000 if quality is good

### Cost Management

1. **Stop VM when not in use**:
   ```bash
   # Stop (keeps disk, stops compute billing)
   gcloud compute instances stop gaussian-splatting-vm --zone=us-west1-b
   
   # Restart later
   gcloud compute instances start gaussian-splatting-vm --zone=us-west1-b
   ```

2. **Use Preemptible VMs for experiments** (up to 80% cheaper):
   - Add `--preemptible` flag when creating VM
   - May be terminated with 30-second warning
   - Good for testing, not for long training runs

3. **Delete when project complete**:
   ```bash
   gcloud compute instances delete gaussian-splatting-vm --zone=us-west1-b
   ```

4. **Estimated Costs** (as of 2024):
   - L4 GPU (G2): ~$0.70-0.90/hour
   - T4 GPU: ~$0.35-0.50/hour
   - Storage (SSD): ~$0.17/GB/month

### Data Organization

Recommended folder structure:
```
gaussian-splatting/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ samples/          # Downloaded test datasets
‚îÇ   ‚îú‚îÄ‚îÄ project_a/        # Your custom scene 1
‚îÇ   ‚îú‚îÄ‚îÄ project_b/        # Your custom scene 2
‚îÇ   ‚îî‚îÄ‚îÄ videos/           # Raw videos (before frame extraction)
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îî‚îÄ‚îÄ [auto-generated model folders]
‚îî‚îÄ‚îÄ local_backups/        # Downloaded models (local machine only)
```

---

## Advanced Topics

### Multi-Scene Training

Train on multiple scenes sequentially:

```bash
#!/bin/bash
# train_batch.sh

scenes=("scene1" "scene2" "scene3")

for scene in "${scenes[@]}"; do
    echo "Training on $scene..."
    python train.py -s data/$scene --eval
    echo "Completed $scene"
done
```

### Custom Training Parameters

Fine-tune training for specific scenarios:

```bash
# High-quality mode (slower, better results)
python train.py -s data/myscene \
    --iterations 50000 \
    --densify_until_iter 25000 \
    --position_lr_init 0.00008

# Fast preview mode
python train.py -s data/myscene \
    --iterations 10000 \
    -r 4 \
    --densify_until_iter 5000

# Large scene optimization
python train.py -s data/myscene \
    --resolution 2 \
    --densify_grad_threshold 0.0001
```

### Network Viewer (Training Monitoring)

The network viewer allows real-time monitoring of training progress:

**On VM**:
```bash
# Training automatically starts network server on port 6009
python train.py -s data/myscene
```

**On Local Machine**:
```bash
# Forward port through SSH tunnel
gcloud compute ssh gaussian-splatting-vm --zone=us-west1-b -- -L 6009:localhost:6009

# Run SIBR remote viewer (if compiled)
./SIBR_remoteGaussian_app
```

### Using Checkpoints

Resume training from a saved checkpoint:

```bash
# Training saves checkpoints automatically at iteration 7000, 30000
ls output/<uuid>/point_cloud/

# Resume from iteration 7000
python train.py -s data/myscene \
    --start_checkpoint output/<uuid>/point_cloud/iteration_7000/point_cloud.ply \
    --iterations 50000
```

---

## Quick Reference: Essential Commands

```bash
# === Setup ===
gcloud compute ssh gaussian-splatting-vm --zone=us-west1-b
conda activate gaussian_splatting

# === Data Processing ===
# Video ‚Üí Images
ffmpeg -i video.mp4 -qscale:v 1 -vf "fps=2" data/scene/input/%04d.jpg

# Images ‚Üí COLMAP
python convert.py -s data/scene

# === Training ===
# Basic
python train.py -s data/scene

# With evaluation split
python train.py -s data/scene --eval

# Reduced resolution (faster)
python train.py -s data/scene -r 2

# === Post-Training ===
# Render views
python render.py -m output/<uuid>

# Calculate metrics
python metrics.py -m output/<uuid>

# === File Transfer ===
# Upload to VM
gcloud compute scp --recurse /local/path/ gaussian-splatting-vm:~/gaussian-splatting/data/

# Download from VM
gcloud compute scp --recurse gaussian-splatting-vm:~/gaussian-splatting/output/<uuid> ./local/

# === VM Management ===
# Stop VM
gcloud compute instances stop gaussian-splatting-vm --zone=us-west1-b

# Start VM
gcloud compute instances start gaussian-splatting-vm --zone=us-west1-b

# Delete VM
gcloud compute instances delete gaussian-splatting-vm --zone=us-west1-b
```

---

## Summary

This guide has covered:
‚úÖ Understanding the Gaussian Splatting project architecture  
‚úÖ Setting up a GPU-enabled GCP VM instance  
‚úÖ Configuring the Python/CUDA environment  
‚úÖ Training workflows for sample, custom image, and video datasets  
‚úÖ Rendering and evaluating results  
‚úÖ Troubleshooting common issues  
‚úÖ Best practices for quality and cost optimization  

**Next Steps**:
1. Follow VM setup section to create your instance
2. Test with sample dataset to verify installation
3. Process your own data following custom workflows
4. Download and visualize results locally

For more information, see:
- [Original Project README](../README.md)
- [Official Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)
- [COLMAP Documentation](https://colmap.github.io/)

**Happy Splatting! üé®**
